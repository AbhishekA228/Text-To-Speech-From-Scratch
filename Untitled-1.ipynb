{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Abhishek\n",
      "[nltk_data]     A\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['printing', ',', 'in', 'the', 'only', 'sense', 'with', 'which', 'we', 'are', 'at', 'present', 'concerned', ',', 'differs', 'from', 'most', 'if', 'not', 'from', 'all', 'the', 'arts', 'and', 'crafts', 'represented', 'in', 'the', 'exhibition', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ','], ['in', 'being', 'comparatively', 'modern.', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ','], ['for', 'although', 'the', 'chinese', 'took', 'impressions', 'from', 'wood', 'blocks', 'engraved', 'in', 'relief', 'for', 'centuries', 'before', 'the', 'woodcutters', 'of', 'the', 'netherlands', ',', 'by', 'a', 'similar', 'process', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ','], ['produced', 'the', 'block', 'books', ',', 'which', 'were', 'the', 'immediate', 'predecessors', 'of', 'the', 'true', 'printed', 'book', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ','], ['the', 'invention', 'of', 'movable', 'metal', 'letters', 'in', 'the', 'middle', 'of', 'the', 'fifteenth', 'century', 'may', 'justly', 'be', 'considered', 'as', 'the', 'invention', 'of', 'the', 'art', 'of', 'printing.', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',']]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, float):\n",
    "        # Handle missing values (NaNs)\n",
    "        return []\n",
    "    else:\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        return tokens\n",
    "\n",
    "metadata_path = r\"C:\\Users\\Abhishek A\\Desktop\\AI&ML\\NLP Projects\\Text To Speech From Scratch\\LJSpeech-1.1\\metadata.csv\"  # Update this with the path to your metadata file\n",
    "metadata = pd.read_csv(metadata_path, sep=\"|\", header=None, names=[\"File\", \"Transcription\"])\n",
    "\n",
    "processed_texts = []\n",
    "for transcription in metadata[\"Transcription\"]:\n",
    "    processed_texts.append(preprocess_text(transcription))\n",
    "\n",
    "# Example usage:\n",
    "print(processed_texts[:5])  # Print processed texts for the first 5 samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio Shape: (212893,)\n",
      "Sample Rate: 22050\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_audio(audio_file, sample_rate=22050):\n",
    "    y, sr = librosa.load(audio_file, sr=sample_rate)\n",
    "    return y, sr\n",
    "\n",
    "# Example usage:\n",
    "audio_file = r\"C:\\Users\\Abhishek A\\Desktop\\AI&ML\\NLP Projects\\Text To Speech From Scratch\\LJSpeech-1.1\\wavs\\LJ001-0001.wav\"\n",
    "audio, sr = preprocess_audio(audio_file)\n",
    "print(\"Audio Shape:\", audio.shape)\n",
    "print(\"Sample Rate:\", sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(TextEncoder, self).__init__()\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        _, hidden = self.rnn(input)\n",
    "        return hidden\n",
    "\n",
    "class MelDecoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MelDecoder, self).__init__()\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        output, _ = self.rnn(input, hidden)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "class TTSModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(TTSModel, self).__init__()\n",
    "        self.encoder = TextEncoder(input_size, hidden_size)\n",
    "        self.decoder = MelDecoder(input_size, hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, text_input):\n",
    "        encoded_text = self.encoder(text_input)\n",
    "        mel_output = self.decoder(text_input, encoded_text)\n",
    "        return mel_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (695626515.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[12], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    def __init__(self, ...):\u001b[0m\n\u001b[1;37m                       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define your TTS model architecture\n",
    "class TTSModel(nn.Module):\n",
    "    def __init__(self, ...):\n",
    "        # Define your model layers here\n",
    "        ...\n",
    "\n",
    "    def forward(self, ...):\n",
    "        # Define the forward pass of your model\n",
    "        ...\n",
    "\n",
    "# Define your custom dataset class\n",
    "class TTSDataset(Dataset):\n",
    "    def __init__(self, metadata_path, audio_dir, tokenizer, max_seq_length):\n",
    "        self.metadata = pd.read_csv(metadata_path, sep=\"|\", header=None, names=[\"File\", \"Transcription\"])\n",
    "        self.audio_dir = audio_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_file = os.path.join(self.audio_dir, self.metadata.iloc[idx, 0] + '.wav')\n",
    "        text = self.metadata.iloc[idx, 1]\n",
    "        \n",
    "        # Tokenize text and convert to numerical representation\n",
    "        tokens = self.tokenizer(text)\n",
    "        # Pad or truncate tokens to max_seq_length\n",
    "        padded_tokens = tokens[:self.max_seq_length] + [0] * (self.max_seq_length - len(tokens))\n",
    "        \n",
    "        # Load and preprocess audio\n",
    "        audio, sr = librosa.load(audio_file, sr=22050)\n",
    "        # Preprocess audio (e.g., convert to mel spectrogram)\n",
    "        # You can use your own audio preprocessing method here\n",
    "        \n",
    "        return torch.tensor(padded_tokens), torch.tensor(audio)\n",
    "\n",
    "# Define your tokenizer function\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training parameters\n",
    "metadata_path = r\"C:\\Users\\Abhishek A\\Desktop\\AI&ML\\NLP Projects\\Text To Speech From Scratch\\LJSpeech-1.1\\metadata.csv\"  # Path to metadata file\n",
    "audio_dir = r\"C:\\Users\\Abhishek A\\Desktop\\AI&ML\\NLP Projects\\Text To Speech From Scratch\\LJSpeech-1.1\\wavs\"    # Path to directory containing audio files\n",
    "tokenizer = preprocess_text              # Tokenizer function\n",
    "max_seq_length = 100                     # Maximum sequence length for text input\n",
    "batch_size = 32                          # Batch size\n",
    "num_epochs = 10                          # Number of epochs\n",
    "learning_rate = 1e-3                     # Learning rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TTSModel(...)\n",
    "\n",
    "# Define your loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Instantiate your dataset and dataloader\n",
    "dataset = TTSDataset(metadata_path, audio_dir, tokenizer, max_seq_length)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for batch_idx, (text_input, audio_target) in enumerate(dataloader):\n",
    "        # Move data to device (e.g., GPU)\n",
    "        text_input, audio_target = text_input.to(device), audio_target.to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        mel_output = model(text_input)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(mel_output, audio_target)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate the total loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    # Print the average loss for the epoch\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(dataloader)}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"tts_model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
